{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-683eJKRKfxl",
        "colab_type": "text"
      },
      "source": [
        "# Sequence-to-sequence\n",
        "\n",
        "본 ipython notebook은 [DIYA](https://blog.diyaml.com/) 회원들의 자연어처리 스터디를 위해, 아래의 자료를 바탕으로 만들어졌습니다.\n",
        "* [Stanford CS224N Assignment 4](http://web.stanford.edu/class/cs224n/assignments/a4.pdf)\n",
        "* [PyTorch Seq2Seq](https://github.com/bentrevett/pytorch-seq2seq)\n",
        "* [네이버 리뷰 데이터를 활용한 한글 데이터 감정 분석](https://github.com/reniew/NSMC_Sentimental-Analysis)\n",
        "* [토치텍스트 튜토리얼(Torchtext tutorial) - 한국어](https://wikidocs.net/65348)\n",
        "\n",
        "본 실습의 구성은 다음과 같습니다.\n",
        "1. [torchtext를 이용한 데이터셋 생성](#Data-Iterator)\n",
        "2. [Seq2Seq 구현](#Seq2Seq-Model)\n",
        "3. [Seq2Seq 학습](#Training-the-Model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT--m0CjNhZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "박규병님께서 Kaggle에 올려주신 1000sents 데이터를 다운로드합니다.\n",
        "torchtext에서의 Field 기능을 활용하기 위해, 전처리는 하지 않았습니다.\n",
        "\"\"\"\n",
        "import requests\n",
        "\n",
        "# Get file link from google drive\n",
        "file_id = \"1R_XGHvkgMArQM6SM59_U7qLaDD2gb3FZ\"\n",
        "file_download_link = \"https://docs.google.com/uc?export=download&id=\" + file_id\n",
        "\n",
        "# Download and unzip file\n",
        "data = requests.get(file_download_link)\n",
        "filename = '1000sents.csv'\n",
        "with open(filename, 'wb') as f:\n",
        "    f.write(data.content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EADcYrzOUSwn",
        "colab_type": "text"
      },
      "source": [
        "## Data Iterator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn8ObciEUWMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "토큰화를 위한 형태소분석기를 정의해줍니다.\n",
        "\"\"\"\n",
        "!pip install konlpy -q\n",
        "!pip install torchtext==0.6.0 -q\n",
        "\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "okt = Okt()  # 이번에는 과거 트위터 형태소 분석기라고 부르던 Okt 분석기를 사용해 보겠습니다.\n",
        "stop_words = [  # 불용어를 정의합니다.\n",
        "    '은', '는', '이', '가', '하', '아', '것', '들', '의', '있', '되', '수',\n",
        "    '보', '주', '등', '한', '을', '를'\n",
        "]\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"code modified from https://github.com/reniew/NSMC_Sentimental-Analysis\"\"\"\n",
        "    # 1. 한글 및 공백을 제외한 문자 모두 제거.\n",
        "    review_text = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]\", \"\", text)\n",
        "\n",
        "    # 2. okt 객체를 활용해서 형태소 단위로 나눈다.\n",
        "    word_review = okt.morphs(review_text, stem=True)\n",
        "\n",
        "    # 3. 불용어 제거\n",
        "    word_review = [token for token in word_review if not token in stop_words]\n",
        "\n",
        "    return word_review"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iezek48KYp2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "torchtext의 데이터셋 형태로 변환하기 위해,\n",
        "데이터의 각 필드를 정의해줍니다.\n",
        "\"\"\"\n",
        "from torchtext import data\n",
        "\n",
        "SRC = data.Field(       # 한국어 문장\n",
        "    tokenize=tokenize,\n",
        "    init_token='<sos>', # 문장의 시작 토큰\n",
        "    eos_token='<eos>',  # 문장의 끝 토큰\n",
        "    include_lengths=True\n",
        ")\n",
        "TRG = data.Field(       # 영어 문장\n",
        "    tokenize='spacy',\n",
        "    init_token='<sos>',\n",
        "    eos_token='<eos>',\n",
        "    lower=True\n",
        ")\n",
        "\n",
        "train_data = data.TabularDataset(\n",
        "    path='1000sents.csv',\n",
        "    format='csv',\n",
        "    fields=[('korean', SRC), ('english', TRG)],\n",
        "    skip_header=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qVj9EUsXJNp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 샘플 데이터를 출력합니다.\n",
        "for i in range(10):\n",
        "    print(train_data[i].korean, '\\t', train_data[i].english)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVvZjVBUZsbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "각기 다른 형태소들을 (distinct words) 숫자로 변환해줍니다.\n",
        "최소 3번 이상 등장한 형태소들에 대해서만 분석을 수행합니다.\n",
        "\"\"\"\n",
        "SRC.build_vocab(train_data, min_freq=3)\n",
        "TRG.build_vocab(train_data, min_freq=3)\n",
        "\n",
        "print('단어 집합의 크기 : 한국어 {}개  영어 {}개'.format(len(SRC.vocab), len(TRG.vocab)))\n",
        "print(list(SRC.vocab.stoi.items()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wa8TDi9abWsq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "torchtext의 BucketIterator 객체를 통해 서로 다른 길이의 문장들을 한번에 불러올 수 있습니다.\n",
        "배치 내에 가장 긴 문장을 첫번째에 두고, 이후의 문장들은 뒤에 <pad> 토큰을 붙임으로써 첫번째 문장과 길이를 맞춰줍니다.\n",
        "\"\"\"\n",
        "import itertools\n",
        "\n",
        "batch_size = 32\n",
        "train_iterator = data.BucketIterator(\n",
        "    train_data, \n",
        "    batch_size=batch_size,\n",
        "    sort_within_batch=True,\n",
        "    sort_key=lambda x : len(x.korean),\n",
        ")\n",
        "\n",
        "# padding이 잘 들어갔는지 확인해봅시다.\n",
        "sample = next(itertools.islice(train_iterator, 4, None)).korean[0]\n",
        "print(sample.shape)\n",
        "print(sample.T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2U9t_SJfru9",
        "colab_type": "text"
      },
      "source": [
        "## Seq2Seq Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndGy1y28gG65",
        "colab_type": "text"
      },
      "source": [
        "![Seq2Seq Overview](https://github.com/bentrevett/pytorch-seq2seq/raw/d876a1dcacd7aeeeeeaff2c9b806d23116df048f/assets/seq2seq7.png)\n",
        "\n",
        "이 notebook에서 구현할 sequence-to-sequence 모델은 attention과 bidirectional GRU를 활용한, [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)에서 소개한 모델입니다.\n",
        "\n",
        "아래 순서를 따라 이 모델의 구성요소들을 하나씩 구현해봅시다.  \n",
        "* Encoder\n",
        "    * Word Embedding (Korean)\n",
        "    * Bidirectional GRU\n",
        "    * Fully Connected Layer\n",
        "* Attention\n",
        "    * Fully Connected Layer\n",
        "    * Masking\n",
        "* Decoder\n",
        "    * Word Embedding (English)\n",
        "    * Unidirectional GRU\n",
        "    * Fully Connected Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLmwJEFWee7f",
        "colab_type": "text"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCdC8f15gKRW",
        "colab_type": "text"
      },
      "source": [
        "![Encoder Overview](https://github.com/bentrevett/pytorch-seq2seq/raw/d876a1dcacd7aeeeeeaff2c9b806d23116df048f/assets/seq2seq8.png)\n",
        "\n",
        "이 모델의 Encoder는 다음의 3가지 부분으로 이루어져있습니다.  \n",
        "* Word Embedding (Korean)\n",
        "* Bidirectional GRU\n",
        "* Fully Connected Layer\n",
        "\n",
        "각 레이어의 형태를 아래 코드와 같이 정의할 때, Bidirectional GRU의 forward hidden state와 backward hidden state를 합쳐 (concatenate) Fully Connected Layer로 전달해주세요.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh-Egl9335tE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO\n",
        "Encoder Module의 forward 함수를 완성해주세요.\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim, encoder_dim, decoder_dim,\n",
        "                 dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential(\n",
        "            nn.Embedding(input_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.rnn = nn.GRU(embed_dim, encoder_dim, bidirectional=True)\n",
        "        self.fc = nn.Linear(encoder_dim * 2, decoder_dim)\n",
        "\n",
        "    def forward(self, src, src_len):\n",
        "        embed = self.embed(src)\n",
        "        \n",
        "        # 각 문장의 길이를 명시해줌으로써 rnn의 hidden state에 불필요한 padding이 포함되지 않도록 합니다.\n",
        "        packed_embed = nn.utils.rnn.pack_padded_sequence(embed, src_len)\n",
        "        packed_outputs, hidden = self.rnn(packed_embed)\n",
        "\n",
        "        # zero-padding을 통해 rnn의 출력값을 다시 동일한 길이로 맞춰줍니다.\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
        "\n",
        "        # TODO: hidden의 0번 dimension을 합쳐서 fully connected로 전달해줍니다.\n",
        "        hidden_cat = None\n",
        "        hidden = torch.tanh(self.fc(hidden_cat))\n",
        "        return outputs, hidden\n",
        "\n",
        "# Tests\n",
        "encoder = Encoder(len(SRC.vocab), 2, 3, 4)\n",
        "for _ in range(3):\n",
        "    src, src_len = next(iter(train_iterator)).korean\n",
        "    outputs, hidden = encoder(src, src_len)\n",
        "    assert outputs.shape == torch.Size((max(src_len), batch_size, 6))\n",
        "    assert hidden.shape == torch.Size((batch_size, 4))\n",
        "print(\"All tests passed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ-OSRAcyfVh",
        "colab_type": "text"
      },
      "source": [
        "### Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztZQ_6bzyiII",
        "colab_type": "text"
      },
      "source": [
        "![Attention Overview](https://github.com/bentrevett/pytorch-seq2seq/raw/d876a1dcacd7aeeeeeaff2c9b806d23116df048f/assets/seq2seq9.png)\n",
        "\n",
        "encoder output을 $H$, 이전 decoder hidden state을 $s_{t-1}$라 할 때 이 모델에서의 attention $a_t$은 다음과 같이 주어집니다.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "E_t &= (\\text{tanh} \\circ \\text{fc})\\left(s_{t-1} \\oplus H\\right) \\\\\n",
        "\\hat{a_t} &= v^T E_t \\\\\n",
        "a_t &= \\text{softmax}(\\hat{a_t})\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "이때 $\\oplus$는 두 벡터의 결합연산(concatenate)을 나타내고, $v$는 $E_t$와 크기가 같은 벡터로 모델이 경사하강을 통해 학습하는 모수입니다.\n",
        "\n",
        "위 수식을 보고 아래의 Attention Module을 구현해주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7zgRlOqo1rb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO\n",
        "Attention Module의 forward 함수를 완성해주세요.\n",
        "\"\"\"\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, encoder_dim, decoder_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(encoder_dim * 2 + decoder_dim, decoder_dim)\n",
        "        self.v = nn.Linear(decoder_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs, mask):\n",
        "        # hidden: [batch_size, decoder_dim]\n",
        "        # encoder_outputs: [max(src_len), batch_size, encoder_dim * 2]\n",
        "        \n",
        "        # TODO: hidden_cat의 크기가 다음과 같이 되도록 두 텐서를 재배열해주세요.\n",
        "        # hidden_cat: [batch_size, max(src_len), encoder_dim * 2 + decoder_dim]\n",
        "        hidden_cat = None\n",
        "\n",
        "        energy = torch.tanh(self.fc(hidden_cat))\n",
        "        attention = self.v(energy).squeeze(-1)\n",
        "\n",
        "        # mask가 False인 곳(padding)은 전부 -inf로 채웁니다.\n",
        "        attention = attention.masked_fill(~mask, -1e10)  \n",
        "        \n",
        "        return torch.softmax(attention, dim=-1)\n",
        "\n",
        "# Tests\n",
        "encoder = Encoder(len(SRC.vocab), 2, 3, 4)\n",
        "attention = Attention(3, 4)\n",
        "pad_idx = SRC.vocab.stoi[SRC.pad_token]\n",
        "for _ in range(3):\n",
        "    src, src_len = next(iter(train_iterator)).korean\n",
        "    outputs, hidden = encoder(src, src_len)\n",
        "    mask = (src != pad_idx).T  # padding이 아닌 곳에만 attention을 계산합니다.\n",
        "    a = attention(hidden, outputs, mask)\n",
        "    assert a.shape == torch.Size((outputs.size(1), max(src_len)))\n",
        "print(\"All tests passed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DdB8HYEfA1I",
        "colab_type": "text"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj1tbNBJgQkT",
        "colab_type": "text"
      },
      "source": [
        "![Decoder Overview](https://github.com/bentrevett/pytorch-seq2seq/raw/d876a1dcacd7aeeeeeaff2c9b806d23116df048f/assets/seq2seq6.png)\n",
        "\n",
        "이 모델의 Decoder는 다음의 3가지 부분으로 이루어져있습니다.  \n",
        "* Word Embedding (English)\n",
        "* Unidirectional GRU\n",
        "* Fully Connected Layer\n",
        "\n",
        "Decoder의 경우 encoder와 달리, RNN에 이전 state $s_{t - 1}$와 더불어 attention을 적용한 encoder의 출력값 $a_t^T H$을 함께 입력해주어야 합니다. 또한 이 모델에서는 최종 출력값을 계산할 때 위 그림과 같이 word embedding과 rnn output, attentioned을 적용한 encoder output을 모두 이용한다는 점을 참조해주세요.\n",
        "\n",
        "각 레이어의 형태를 아래 코드와 같이 정의할 때, decoder module의 forward 함수를 완성해주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4cmhNbajoq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO\n",
        "Decoder Module의 forward 함수를 완성해주세요.\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, embed_dim, encoder_dim, decoder_dim,\n",
        "                 dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential(\n",
        "            nn.Embedding(output_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.rnn = nn.GRU(encoder_dim * 2 + embed_dim, decoder_dim)\n",
        "        self.fc = nn.Linear(\n",
        "            encoder_dim * 2 + embed_dim + decoder_dim,\n",
        "            output_dim\n",
        "        )\n",
        "\n",
        "    def forward(self, trg, hidden, attended_encoder_outputs):\n",
        "        embed = self.embed(trg.unsqueeze(0))\n",
        "\n",
        "        # TODO: embedding과 attended_outputs를 합쳐서 (concatenate) rnn에 전달해줍니다.\n",
        "        output, hidden = self.rnn(None, None)\n",
        "\n",
        "        # 모두 길이가 1이므로 0번 dimension을 제거해줍니다.\n",
        "        embed = embed.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        hidden = hidden.squeeze(0)\n",
        "        attended = attended_encoder_outputs.squeeze(0)\n",
        "        \n",
        "        # TODO: embed, output, attended를 모두 이용해 최종 출력값을 계산합니다.\n",
        "        output_cat = None\n",
        "        prediction = self.fc(output_cat)\n",
        "\n",
        "        return prediction, hidden\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "앞서 작성한 module들을 합쳐 Seq2Seq module을 정의합니다.\n",
        "\"\"\"\n",
        "import random\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, embed_dim,\n",
        "                 encoder_dim, decoder_dim, dropout=0.5, pad_idx=1):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(input_dim, embed_dim, encoder_dim, decoder_dim,\n",
        "                               dropout=dropout)\n",
        "        self.attention = Attention(encoder_dim, decoder_dim)\n",
        "        self.decoder = Decoder(output_dim, embed_dim, encoder_dim, decoder_dim,\n",
        "                               dropout=dropout)\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def forward(self, src, src_len, trg, teacher_force=0.5):\n",
        "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
        "        mask = (src != self.pad_idx).T\n",
        "        a = self.attention(hidden, encoder_outputs, mask)\n",
        "        attended = (a.T.unsqueeze(-1) * encoder_outputs).sum(0, keepdims=True)\n",
        "        \n",
        "        # 각 시점에서 output의 확률을 저장할 placeholder를 생성합니다.\n",
        "        outputs = torch.zeros(*trg.shape, len(TRG.vocab))\n",
        "        input = trg[0]\n",
        "        for t in range(1, trg.size(0)):\n",
        "            output, hidden = self.decoder(input, hidden, attended)\n",
        "            outputs[t] = output\n",
        "\n",
        "            # Teacher forcing을 일정 확률로 적용합니다.\n",
        "            if random.random() < teacher_force:\n",
        "                input = trg[t]\n",
        "            else:\n",
        "                input = output.argmax(1)\n",
        "        \n",
        "        return outputs\n",
        "\n",
        "\n",
        "# Tests\n",
        "pad_idx = SRC.vocab.stoi[SRC.pad_token]\n",
        "seq2seq = Seq2Seq(len(SRC.vocab), len(TRG.vocab), 2, 3, 4, pad_idx=pad_idx)\n",
        "for _ in range(3):\n",
        "    sent_pair = next(iter(train_iterator))\n",
        "    src, src_len = sent_pair.korean\n",
        "    trg = sent_pair.english\n",
        "    outputs = seq2seq(src, src_len, trg)\n",
        "    assert not (outputs[1:] == 0).any()\n",
        "print(\"All tests passed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NjxA4WefClH",
        "colab_type": "text"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuyIJtjbgYdV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "CrossEntropyLoss를 이용해 모델을 학습시킵니다.\n",
        "Validation dataset을 정의하지 않았으므로 따로 evaluation은 하지 않습니다.\n",
        "\"\"\"\n",
        "import time\n",
        "import math\n",
        "\n",
        "src_pad_idx = SRC.vocab.stoi[SRC.pad_token]\n",
        "trg_pad_idx = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "def train(model, iterator, epochs=20, lr=1e-3, teacher_force=0.5):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\n",
        "    tmp = \"Epoch: {:3d} | Time: {:.4f} ms | Loss: {:.4f} | PPL: {:.4f}\"\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "        \n",
        "        avg_loss = 0\n",
        "        avg_bleu_score = 0\n",
        "        for batch in iterator:\n",
        "            src, src_len = batch.korean\n",
        "            trg = batch.english\n",
        "\n",
        "            # Loss를 계산합니다.\n",
        "            model.train()\n",
        "            outputs = model(src, src_len, trg, teacher_force=teacher_force)\n",
        "            outputs = outputs[1:].view(-1, outputs.size(-1))\n",
        "            loss = criterion(outputs, trg[1:].view(-1))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            avg_loss += loss.item()\n",
        "        \n",
        "        avg_loss /= len(iterator)\n",
        "        elapsed = time.time() - start_time\n",
        "        print(tmp.format(epoch + 1, elapsed, avg_loss, math.exp(avg_loss)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP93Jued2SwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모델을 학습시켜봅시다.\n",
        "model = Seq2Seq(len(SRC.vocab), len(TRG.vocab), 32, 64, 64,\n",
        "                pad_idx=src_pad_idx)\n",
        "train(model, train_iterator)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}