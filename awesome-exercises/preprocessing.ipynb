{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBgnWfhXLXlS",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing\n",
        "\n",
        "본 ipython notebook은 [DIYA](https://blog.diyaml.com/) 회원들의 자연어처리 스터디를 위해, 아래의 자료를 바탕으로 만들어졌습니다.\n",
        "* [Stanford CS224N Assignment 1](http://web.stanford.edu/class/cs224n/assignments/a1_preview/exploring_word_vectors.html)\n",
        "* [ratsgo님의 한국어 임베딩 튜토리얼](https://ratsgo.github.io/embedding/)\n",
        "\n",
        "본 실습의 구성은 다음과 같습니다.\n",
        "1. [한국어 위키피디아 데이터 다운로드](#Corpus-Data)\n",
        "2. [은전한닢 형태소 분석기로 전처리](#Preprocessing)\n",
        "3. [co-occurence matrix 계산](#Co-Occurrence-Matrix)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qq3l2tF_DoU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 한글 폰트 설치\n",
        "!apt -qq -y install fonts-nanum\n",
        " \n",
        "# matplotlib 한글 폰트 설정\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
        "font = fm.FontProperties(fname=fontpath)\n",
        "plt.rc('font', family='NanumBarunGothic') \n",
        "mpl.font_manager._rebuild()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nOWNsaUGZKX",
        "colab_type": "text"
      },
      "source": [
        "위 셀을 실행하신 뒤 런타임을 한번만 다시 시작해주세요 :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt8WahCXPhYW",
        "colab_type": "text"
      },
      "source": [
        "## Corpus Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OMlfHkQW6X_",
        "colab_type": "text"
      },
      "source": [
        "### Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHmf0xPjQSxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import requests\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "def download(url, filename):\n",
        "    \"\"\"Helper function for downloading files with progress bar.\n",
        "    code modified from https://stackoverflow.com/questions/15644964/python-progress-bar-and-downloads\n",
        "    \"\"\"\n",
        "    with open(filename, 'wb') as f:\n",
        "        response = requests.get(url, stream=True)\n",
        "        total = response.headers.get('content-length')\n",
        "\n",
        "        if total is None:\n",
        "            f.write(response.content)\n",
        "        else:\n",
        "            total = int(total)\n",
        "            pbar = tqdm(total=total)\n",
        "            for data in response.iter_content(chunk_size=max(int(total/1000), 1024*1024)):\n",
        "                f.write(data)\n",
        "                pbar.update(len(data))\n",
        "\n",
        "\n",
        "# Download ko-wikipedia corpus\n",
        "url = \"https://dumps.wikimedia.org/kowiki/latest/kowiki-latest-pages-articles.xml.bz2\"\n",
        "filename = \"kowiki.xml.bz2\"\n",
        "download(url, filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cirJGzdutNzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bz2\n",
        "\n",
        "# Print sample data\n",
        "with bz2.BZ2File(filename, \"r\") as f:\n",
        "    for idx, line in enumerate(f):\n",
        "        if idx >= 100:\n",
        "            break\n",
        "        print(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOmlitO_XAn1",
        "colab_type": "text"
      },
      "source": [
        "### Create corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWrVIKrcWfh6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from gensim.corpora import WikiCorpus, Dictionary\n",
        "from gensim.utils import to_unicode\n",
        "\n",
        "\n",
        "WIKI_REMOVE_CHARS = re.compile(\"'+|(=+.{2,30}=+)|__TOC__|(ファイル:).+|:(en|de|it|fr|es|kr|zh|no|fi):|\\n\", re.UNICODE)\n",
        "WIKI_SPACE_CHARS = re.compile(\"(\\\\s|゙|゚|　)+\", re.UNICODE)\n",
        "EMAIL_PATTERN = re.compile(\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\", re.UNICODE)\n",
        "URL_PATTERN = re.compile(\"(ftp|http|https)?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", re.UNICODE)\n",
        "WIKI_REMOVE_TOKEN_CHARS = re.compile(\"(\\\\*$|:$|^파일:.+|^;)\", re.UNICODE)\n",
        "MULTIPLE_SPACES = re.compile(' +', re.UNICODE)\n",
        "\n",
        "\n",
        "def tokenize(content, token_min_len=2, token_max_len=100, lower=True):\n",
        "    \"\"\"Remove unnecessary tokens.\n",
        "    code from https://github.com/ratsgo/embedding/blob/master/preprocess/dump.py\n",
        "    \"\"\"\n",
        "    content = re.sub(EMAIL_PATTERN, ' ', content)  # remove email pattern\n",
        "    content = re.sub(URL_PATTERN, ' ', content) # remove url pattern\n",
        "    content = re.sub(WIKI_REMOVE_CHARS, ' ', content)  # remove unnecessary chars\n",
        "    content = re.sub(WIKI_SPACE_CHARS, ' ', content)\n",
        "    content = re.sub(MULTIPLE_SPACES, ' ', content)\n",
        "    tokens = content.replace(\", )\", \"\").split(\" \")\n",
        "    result = []\n",
        "    for token in tokens:\n",
        "        if not token.startswith('_'):\n",
        "            token_candidate = to_unicode(re.sub(WIKI_REMOVE_TOKEN_CHARS, '', token))\n",
        "        else:\n",
        "            token_candidate = \"\"\n",
        "        if len(token_candidate) > 0:\n",
        "            result.append(token_candidate)\n",
        "    return result\n",
        "\n",
        "\n",
        "def make_corpus(in_f, out_f):\n",
        "    \"\"\"Convert Wikipedia xml dump file to text corpus.\n",
        "    code from https://github.com/ratsgo/embedding/blob/master/preprocess/dump.py\n",
        "    \"\"\"\n",
        "    output = open(out_f, 'w', encoding=\"utf-8\")\n",
        "    wiki = WikiCorpus(in_f, tokenizer_func=tokenize, dictionary=Dictionary())\n",
        "    pbar = tqdm(total=NUM_ARTICLES)\n",
        "    for idx, text in enumerate(wiki.get_texts()):\n",
        "        output.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '\\n')\n",
        "        pbar.update(1)\n",
        "        if idx >= NUM_ARTICLES:\n",
        "            break\n",
        "    output.close()\n",
        "\n",
        "# Create corpus from raw data\n",
        "NUM_ARTICLES = 10000\n",
        "make_corpus(filename, 'processed.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMIm10SWhs2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print sample data\n",
        "with open('processed.txt', 'r') as f:\n",
        "    data = f.read()\n",
        "    print(data[:1000].replace('.', '.\\n'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pL_MJEyJPuYd",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLbRotYHjEpi",
        "colab_type": "text"
      },
      "source": [
        "### Install Mecab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS1JblCojGSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install konlpy\n",
        "!pip install konlpy\n",
        "\n",
        "# Install JDK and JPype\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip3 install JPype1-py3\n",
        "\n",
        "# Install mecab\n",
        "import os\n",
        "os.chdir('/tmp/')\n",
        "!curl -LO https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.1.tar.gz\n",
        "!tar zxfv mecab-0.996-ko-0.9.1.tar.gz\n",
        "os.chdir('/tmp/mecab-0.996-ko-0.9.1')\n",
        "!./configure\n",
        "!make\n",
        "!make check\n",
        "!make install\n",
        "\n",
        "# Install automake\n",
        "os.chdir('/tmp')\n",
        "!curl -LO http://ftpmirror.gnu.org/automake/automake-1.11.tar.gz\n",
        "!tar -zxvf automake-1.11.tar.gz\n",
        "os.chdir('/tmp/automake-1.11')\n",
        "!./configure\n",
        "!make\n",
        "!make install\n",
        "\n",
        "# Install mecab-ko-dic\n",
        "os.chdir('/tmp')\n",
        "!curl -LO https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.0.1-20150920.tar.gz\n",
        "!tar -zxvf mecab-ko-dic-2.0.1-20150920.tar.gz\n",
        "os.chdir('/tmp/mecab-ko-dic-2.0.1-20150920')\n",
        "!ldconfig\n",
        "!ldconfig -p | grep /usr/local/lib\n",
        "!./autogen.sh\n",
        "!./configure\n",
        "!make\n",
        "!make install\n",
        "\n",
        "# Install mecab-python\n",
        "os.chdir('/content')\n",
        "!git clone https://bitbucket.org/eunjeon/mecab-python-0.996.git\n",
        "os.chdir('/content/mecab-python-0.996')\n",
        "\n",
        "!python setup.py build\n",
        "!python setup.py install\n",
        "os.chdir('/content')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSI58KhYZ-DF",
        "colab_type": "text"
      },
      "source": [
        "### Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDPtVUb6Z8y9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from konlpy.tag import Mecab\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "def tokenize(corpus_fname, output_fname):\n",
        "    \"\"\"Tokenize corpus into morphemes.\n",
        "    code modified from https://github.com/ratsgo/embedding/blob/master/preprocess/supervised_nlputils.py\n",
        "    \"\"\"\n",
        "    tokenizer = Mecab()\n",
        "    with open(corpus_fname, 'r', encoding='utf-8') as f1, \\\n",
        "            open(output_fname, 'w', encoding='utf-8') as f2:\n",
        "        for line in tqdm(list(f1)):\n",
        "            sentence = line.replace('\\n', '').strip()\n",
        "            tokens = tokenizer.pos(sentence)\n",
        "            # 어미, 조사, 기호 등의 불용어를 제거합니다\n",
        "            morphs = []\n",
        "            for morph, tag in tokens:\n",
        "                if tag[0] in ['M', 'N', 'V', 'X']:\n",
        "                    morphs.append(morph)\n",
        "            tokenized_sent = ' '.join(morphs)\n",
        "            f2.writelines(tokenized_sent + '\\n')\n",
        "\n",
        "\n",
        "# Tokenize corpus\n",
        "tokenize('processed.txt', 'morphemes.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-xiZ-ndmeb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print sample data\n",
        "with open('morphemes.txt', 'r') as f:\n",
        "    data = f.read()\n",
        "    print(data[:1000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwZb_M9wP9EM",
        "colab_type": "text"
      },
      "source": [
        "## Co-Occurrence Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pilDPUr6n1nG",
        "colab_type": "text"
      },
      "source": [
        "### `distinct_words`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvAVMvvXbphT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Implementing distinct_words from the assignment of CS224N.\n",
        "code from http://web.stanford.edu/class/cs224n/assignments/a1_preview/exploring_word_vectors.html\n",
        "\"\"\"\n",
        "def distinct_words(corpus):\n",
        "    \"\"\" Determine a list of distinct words for the corpus.\n",
        "        Params:\n",
        "            corpus (list of list of strings): corpus of documents\n",
        "        Return:\n",
        "            corpus_words (list of strings): list of distinct words across the corpus, sorted (using python 'sorted' function)\n",
        "            num_corpus_words (integer): number of distinct words across the corpus\n",
        "    \"\"\"    \n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "    \n",
        "    \n",
        "    # ------------------\n",
        "\n",
        "    return corpus_words, num_corpus_words\n",
        "\n",
        "\n",
        "# ---------------------\n",
        "# Run this sanity check\n",
        "# Note that this not an exhaustive check for correctness.\n",
        "# ---------------------\n",
        "\n",
        "START_TOKEN = '<START>'\n",
        "END_TOKEN = '<END>'\n",
        "\n",
        "# Define toy corpus\n",
        "test_corpus = [\"{} All that glitters isn't gold {}\".format(START_TOKEN, END_TOKEN).split(\" \"), \"{} All's well that ends well {}\".format(START_TOKEN, END_TOKEN).split(\" \")]\n",
        "test_corpus_words, num_corpus_words = distinct_words(test_corpus)\n",
        "\n",
        "# Correct answers\n",
        "ans_test_corpus_words = sorted([START_TOKEN, \"All\", \"ends\", \"that\", \"gold\", \"All's\", \"glitters\", \"isn't\", \"well\", END_TOKEN])\n",
        "ans_num_corpus_words = len(ans_test_corpus_words)\n",
        "\n",
        "# Test correct number of words\n",
        "assert(num_corpus_words == ans_num_corpus_words), \"Incorrect number of distinct words. Correct: {}. Yours: {}\".format(ans_num_corpus_words, num_corpus_words)\n",
        "\n",
        "# Test correct words\n",
        "assert (test_corpus_words == ans_test_corpus_words), \"Incorrect corpus_words.\\nCorrect: {}\\nYours:   {}\".format(str(ans_test_corpus_words), str(test_corpus_words))\n",
        "\n",
        "# Print Success\n",
        "print (\"-\" * 80)\n",
        "print(\"Passed All Tests!\")\n",
        "print (\"-\" * 80)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0bkQalHwK-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try it on ko-wikipedia!\n",
        "with open('morphemes.txt', 'r') as f:\n",
        "    corpus = [f.read().split(\" \")]\n",
        "    corpus_words, num_corpus_words = distinct_words(corpus)\n",
        "print(corpus_words[:100])\n",
        "print(num_corpus_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrNeE8nBsnA_",
        "colab_type": "text"
      },
      "source": [
        "### `compute_co_occurrence_matrix`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55IK5IYKsqdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Implementing compute_co_occurrence_matrix from the assignment of CS224N.\n",
        "code from http://web.stanford.edu/class/cs224n/assignments/a1_preview/exploring_word_vectors.html\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_co_occurrence_matrix(corpus, window_size=4):\n",
        "    \"\"\" Compute co-occurrence matrix for the given corpus and window_size (default of 4).\n",
        "    \n",
        "        Note: Each word in a document should be at the center of a window. Words near edges will have a smaller\n",
        "              number of co-occurring words.\n",
        "              \n",
        "              For example, if we take the document \"<START> All that glitters is not gold <END>\" with window size of 4,\n",
        "              \"All\" will co-occur with \"<START>\", \"that\", \"glitters\", \"is\", and \"not\".\n",
        "    \n",
        "        Params:\n",
        "            corpus (list of list of strings): corpus of documents\n",
        "            window_size (int): size of context window\n",
        "        Return:\n",
        "            M (a symmetric numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)): \n",
        "                Co-occurence matrix of word counts. \n",
        "                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.\n",
        "            word2Ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.\n",
        "    \"\"\"\n",
        "    words, num_words = distinct_words(corpus)\n",
        "    M = None\n",
        "    word2Ind = {}\n",
        "    \n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "    \n",
        "    \n",
        "    # ------------------\n",
        "\n",
        "    return M, word2Ind\n",
        "\n",
        "\n",
        "# ---------------------\n",
        "# Run this sanity check\n",
        "# Note that this is not an exhaustive check for correctness.\n",
        "# ---------------------\n",
        "\n",
        "# Define toy corpus and get student's co-occurrence matrix\n",
        "test_corpus = [\"{} All that glitters isn't gold {}\".format(START_TOKEN, END_TOKEN).split(\" \"), \"{} All's well that ends well {}\".format(START_TOKEN, END_TOKEN).split(\" \")]\n",
        "M_test, word2Ind_test = compute_co_occurrence_matrix(test_corpus, window_size=1)\n",
        "\n",
        "# Correct M and word2Ind\n",
        "M_test_ans = np.array( \n",
        "    [[0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,],\n",
        "     [0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,],\n",
        "     [0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,],\n",
        "     [0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,],\n",
        "     [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,],\n",
        "     [0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,],\n",
        "     [1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,],\n",
        "     [0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,],\n",
        "     [0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,],\n",
        "     [1., 0., 0., 1., 1., 0., 0., 0., 1., 0.,]]\n",
        ")\n",
        "ans_test_corpus_words = sorted([START_TOKEN, \"All\", \"ends\", \"that\", \"gold\", \"All's\", \"glitters\", \"isn't\", \"well\", END_TOKEN])\n",
        "word2Ind_ans = dict(zip(ans_test_corpus_words, range(len(ans_test_corpus_words))))\n",
        "\n",
        "# Test correct word2Ind\n",
        "assert (word2Ind_ans == word2Ind_test), \"Your word2Ind is incorrect:\\nCorrect: {}\\nYours: {}\".format(word2Ind_ans, word2Ind_test)\n",
        "\n",
        "# Test correct M shape\n",
        "assert (M_test.shape == M_test_ans.shape), \"M matrix has incorrect shape.\\nCorrect: {}\\nYours: {}\".format(M_test.shape, M_test_ans.shape)\n",
        "\n",
        "# Test correct M values\n",
        "for w1 in word2Ind_ans.keys():\n",
        "    idx1 = word2Ind_ans[w1]\n",
        "    for w2 in word2Ind_ans.keys():\n",
        "        idx2 = word2Ind_ans[w2]\n",
        "        student = M_test[idx1, idx2]\n",
        "        correct = M_test_ans[idx1, idx2]\n",
        "        if student != correct:\n",
        "            print(\"Correct M:\")\n",
        "            print(M_test_ans)\n",
        "            print(\"Your M: \")\n",
        "            print(M_test)\n",
        "            raise AssertionError(\"Incorrect count at index ({}, {})=({}, {}) in matrix M. Yours has {} but should have {}.\".format(idx1, idx2, w1, w2, student, correct))\n",
        "\n",
        "# Print Success\n",
        "print (\"-\" * 80)\n",
        "print(\"Passed All Tests!\")\n",
        "print (\"-\" * 80)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiAG1bqE13NO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try it on ko-wikipedia!\n",
        "with open('morphemes.txt', 'r') as f:\n",
        "    corpus = [f.read().split(\" \")[:1000]]\n",
        "    M, word2Ind = compute_co_occurrence_matrix(corpus)\n",
        "print(word2Ind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUS0uwEWsqqx",
        "colab_type": "text"
      },
      "source": [
        "### `reduce_to_k_dim`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uRuiUcFsvSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Implementing reduce_to_k_dim from the assignment of CS224N.\n",
        "code from http://web.stanford.edu/class/cs224n/assignments/a1_preview/exploring_word_vectors.html\n",
        "\"\"\"\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "\n",
        "def reduce_to_k_dim(M, k=2):\n",
        "    \"\"\" Reduce a co-occurence count matrix of dimensionality (num_corpus_words, num_corpus_words)\n",
        "        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:\n",
        "            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
        "    \n",
        "        Params:\n",
        "            M (numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)): co-occurence matrix of word counts\n",
        "            k (int): embedding size of each word after dimension reduction\n",
        "        Return:\n",
        "            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings.\n",
        "                    In terms of the SVD from math class, this actually returns U * S\n",
        "    \"\"\"    \n",
        "    n_iters = 10     # Use this parameter in your call to `TruncatedSVD`\n",
        "    M_reduced = None\n",
        "    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n",
        "    \n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "    \n",
        "\n",
        "    # ------------------\n",
        "\n",
        "    print(\"Done.\")\n",
        "    return M_reduced\n",
        "\n",
        "\n",
        "# ---------------------\n",
        "# Run this sanity check\n",
        "# Note that this is not an exhaustive check for correctness \n",
        "# In fact we only check that your M_reduced has the right dimensions.\n",
        "# ---------------------\n",
        "\n",
        "# Define toy corpus and run student code\n",
        "test_corpus = [\"{} All that glitters isn't gold {}\".format(START_TOKEN, END_TOKEN).split(\" \"), \"{} All's well that ends well {}\".format(START_TOKEN, END_TOKEN).split(\" \")]\n",
        "M_test, word2Ind_test = compute_co_occurrence_matrix(test_corpus, window_size=1)\n",
        "M_test_reduced = reduce_to_k_dim(M_test, k=2)\n",
        "\n",
        "# Test proper dimensions\n",
        "assert (M_test_reduced.shape[0] == 10), \"M_reduced has {} rows; should have {}\".format(M_test_reduced.shape[0], 10)\n",
        "assert (M_test_reduced.shape[1] == 2), \"M_reduced has {} columns; should have {}\".format(M_test_reduced.shape[1], 2)\n",
        "\n",
        "# Print Success\n",
        "print (\"-\" * 80)\n",
        "print(\"Passed All Tests!\")\n",
        "print (\"-\" * 80)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udLF0zk5sxXL",
        "colab_type": "text"
      },
      "source": [
        "### `plot_embeddings`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0pxWUlYs2Iv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Implementing reduce_to_k_dim from the assignment of CS224N.\n",
        "code from http://web.stanford.edu/class/cs224n/assignments/a1_preview/exploring_word_vectors.html\n",
        "\"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_embeddings(M_reduced, word2Ind, words):\n",
        "    \"\"\" Plot in a scatterplot the embeddings of the words specified in the list \"words\".\n",
        "        NOTE: do not plot all the words listed in M_reduced / word2Ind.\n",
        "        Include a label next to each point.\n",
        "        \n",
        "        Params:\n",
        "            M_reduced (numpy matrix of shape (number of unique words in the corpus , k)): matrix of k-dimensioal word embeddings\n",
        "            word2Ind (dict): dictionary that maps word to indices for matrix M\n",
        "            words (list of strings): words whose embeddings we want to visualize\n",
        "    \"\"\"\n",
        "\n",
        "    x_coords = M_reduced[:, 0]\n",
        "    y_coords = M_reduced[:, 1]\n",
        "    \n",
        "    for word in words:\n",
        "        idx = word2Ind[word]\n",
        "        embedding = M_reduced[idx]\n",
        "        x = embedding[0]\n",
        "        y = embedding[1]\n",
        "        \n",
        "        plt.scatter(x, y, marker='x', color='red')\n",
        "        plt.text(x, y, word, fontsize=15)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD5ZRE-G9pPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try it on ko-wikipedia!\n",
        "with open('morphemes.txt', 'r') as f:\n",
        "    corpus = [f.read().split(\" \")[100:120]]\n",
        "M, word2Ind = compute_co_occurrence_matrix(corpus)\n",
        "M_reduced = reduce_to_k_dim(M, k=2)\n",
        "\n",
        "# Rescale (normalize) the rows to make them each of unit-length\n",
        "M_lengths = np.linalg.norm(M_reduced, axis=1)\n",
        "M_normalized = M_reduced / M_lengths[:, np.newaxis] # broadcasting\n",
        "\n",
        "words = list(word2Ind.keys())\n",
        "plt.rcParams['figure.figsize'] = (12, 9)\n",
        "plot_embeddings(M_normalized, word2Ind, words)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}