{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epih5RqzeY7s",
        "colab_type": "text"
      },
      "source": [
        "# Transformer\n",
        "\n",
        "본 ipython notebook은 [DIYA](https://blog.diyaml.com/) 회원들의 자연어처리 스터디를 위해, 아래의 자료를 바탕으로 만들어졌습니다.\n",
        "* [Transformer Time Series Prediction](https://github.com/oliverguhr/transformer-time-series-prediction)\n",
        "* [Transformers for Time Series](https://github.com/maxjcohen/transformer)\n",
        "* [Sequence-to-Sequence Modeling with nn.Transformer and TorchText](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)\n",
        "\n",
        "본 실습의 구성은 다음과 같습니다.\n",
        "1. [가상 데이터 생성하기](#Generate-Data)\n",
        "2. [Transformer 구현](#Transformer-Model)\n",
        "3. [Transformer 학습](#Train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTSLvbFKfMgw",
        "colab_type": "text"
      },
      "source": [
        "## Generate Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yItTzB8XfS6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "3가지 조화 진동자(harmonic oscillator)로 이루어진 가상 데이터를 생성합니다.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "class ToyDataset(Dataset):\n",
        "    def __init__(self, seq, hist_window, output_window):\n",
        "        super().__init__()\n",
        "        self.seq = seq\n",
        "        self.hw = hist_window\n",
        "        self.ow = output_window\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.seq) - self.hw - self.ow\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        hist = self.seq[idx:idx + self.hw, None]\n",
        "        preds = self.seq[idx + self.hw:idx + self.hw + self.ow, None]\n",
        "        return torch.FloatTensor(hist), torch.FloatTensor(preds)\n",
        "\n",
        "\n",
        "def get_data(n_samples=3000, test_ratio=0.1, hist_window=90, output_window=7):\n",
        "    time = np.linspace(0, 400, n_samples)\n",
        "    amplitude = np.linspace(0, 5, n_samples)  # linear trend\n",
        "    amplitude += np.sin(time) + np.sin(time * 0.05)\n",
        "    amplitude += np.sin(time * 0.12) * np.random.normal(-0.2, 0.2, len(time))\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "    amplitude = scaler.fit_transform(amplitude.reshape(-1, 1)).reshape(-1)\n",
        "\n",
        "    num_test = int(n_samples * test_ratio)\n",
        "    train_data = ToyDataset(amplitude[:-num_test], hist_window, output_window)\n",
        "    test_data = ToyDataset(amplitude[-num_test:], hist_window, output_window)\n",
        "    return train_data, test_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLEUTSEY0Di-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 가상 데이터의 형태를 그려봅시다.\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "train_data, test_data = get_data()\n",
        "n_samples = len(train_data.seq) + len(test_data.seq)\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(np.arange(0, len(train_data.seq)), train_data.seq, label='train')\n",
        "plt.plot(np.arange(len(train_data.seq), n_samples), test_data.seq, label='test')\n",
        "plt.legend(loc='upper left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFaNPLUfjp7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Transformer를 학습시키기 위한 함수를 정의합니다.\n",
        "\"\"\"\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train(model, epochs=20, lr=1e-3, batch_size=32):\n",
        "    train_data, test_data = get_data()\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    optim = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    tmp = \"Epoch: {:3d} | Time: {:.4f} ms | Loss/Train: {:.4f} | Loss/Eval: {:.4f}\"\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Train single epoch\n",
        "        loss_train = train_epoch(train_loader, model, optim)\n",
        "\n",
        "        # Evaluate\n",
        "        loss_eval = evaluate(test_loader, model)\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        print(tmp.format(epoch + 1, elapsed, loss_train, loss_eval))\n",
        "\n",
        "\n",
        "def train_epoch(dataloader, model, optim):\n",
        "    model.train()\n",
        "    avg_loss = 0.0\n",
        "    for data, targets in dataloader:\n",
        "        outputs = model(data)[:, -targets.size(1):]\n",
        "        loss = F.mse_loss(outputs, targets)\n",
        "        avg_loss += loss.item()\n",
        "\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "    return avg_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def evaluate(dataloader, model):\n",
        "    model.eval()\n",
        "    avg_loss = 0.0\n",
        "    for data, targets in dataloader:\n",
        "        outputs = model(data)[:, -targets.size(1):]\n",
        "        loss = F.mse_loss(outputs, targets)\n",
        "        avg_loss += loss.item()\n",
        "    return avg_loss / len(dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvVwXYPFqgmj",
        "colab_type": "text"
      },
      "source": [
        "## Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hflb6VjSqh4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO\n",
        "torch.nn.TransformerEncoder를 이용해 Transformer Model을 구현해주세요.\n",
        "\"\"\"\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fTzuTy5sxCk",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdNUpGDPsyKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO\n",
        "model을 정의하고 학습시킵니다.\n",
        "\"\"\"\n",
        "model = TransformerModel(None)\n",
        "train(model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}