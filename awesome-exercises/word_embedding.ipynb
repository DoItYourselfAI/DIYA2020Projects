{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word_embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBgnWfhXLXlS",
        "colab_type": "text"
      },
      "source": [
        "# Word Embedding\n",
        "\n",
        "본 ipython notebook은 [DIYA](https://blog.diyaml.com/) 회원들의 자연어처리 스터디를 위해, 아래의 자료를 바탕으로 만들어졌습니다.\n",
        "* [Stanford CS224N Assignment 2](http://web.stanford.edu/class/cs224n/assignments/a2.pdf)\n",
        "* [ratsgo님의 한국어 임베딩 튜토리얼](https://ratsgo.github.io/embedding/)\n",
        "* [딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/book/2155)\n",
        "* [Implementing word2vec in PyTorch (skip-gram model)](https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb)\n",
        "* [Pytorch Global Vectors for Word Representation](https://github.com/kefirski/pytorch_GloVe/blob/master/GloVe/glove.py)\n",
        "\n",
        "본 실습의 구성은 다음과 같습니다.\n",
        "1. [Integer Encoding](#Integer-Encoding)\n",
        "2. [Word2Vec 구현](#Word2Vec)\n",
        "3. [GloVe 구현](#GloVe)\n",
        "4. [비교 및 시각화](#Visualization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcTcPiC0lUfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 한글 폰트 설치\n",
        "!apt -qq -y install fonts-nanum\n",
        " \n",
        "# matplotlib 한글 폰트 설정\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
        "font = fm.FontProperties(fname=fontpath)\n",
        "plt.rc('font', family='NanumBarunGothic')\n",
        "mpl.font_manager._rebuild()\n",
        "plt.rcParams['figure.figsize'] = [15, 10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OPMbQa-lWmn",
        "colab_type": "text"
      },
      "source": [
        "런타임을 다시 시작한 뒤, 위 셀을 한번 더 실행해주세요 :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzmYC7fc11Vm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "전처리한 Naver Sentiment Movie Corpus의 일부를 다운로드합니다.\n",
        "불용어는 적용하지 않고, 꼬꼬마(KKMA)의 형태소 분석 기능만을 이용해 전처리하였습니다.\n",
        "\"\"\"\n",
        "import requests\n",
        "\n",
        "# Get file link from google drive\n",
        "file_share_link = \"https://drive.google.com/open?id=1r1CtcMOQ7sUNma2V5vqrRxrNZojqCQrY\"\n",
        "file_id = file_share_link[file_share_link.find(\"=\") + 1:]\n",
        "file_download_link = \"https://docs.google.com/uc?export=download&id=\" + file_id\n",
        "\n",
        "# Download file\n",
        "data = requests.get(file_download_link)\n",
        "filename = 'nsmc_ratings.txt'\n",
        "with open(filename, 'wb') as f:\n",
        "    f.write(data.content)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8p3tqzeaNgx",
        "colab_type": "text"
      },
      "source": [
        "## Integer Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP6bWmhJc5eD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 텍스트 형태의 말뭉치를 이중 리스트로 변환합니다.\n",
        "corpus = []\n",
        "with open(filename, 'r') as f:\n",
        "    for line in f.readlines():\n",
        "        corpus.append(line.strip(' \\n').split(' '))\n",
        "\n",
        "# 샘플 데이터를 출력합니다.\n",
        "print(corpus[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2dI4G9Ceh9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO\n",
        "vocab 변수가 말뭉치에 존재하는 각기 다른 형태소들을 (distinct words) key로,\n",
        "해당 형태소가 말뭉치 안에 몇 개 들어있는지를 value로 가지도록 구현해주세요.\n",
        "\"\"\"\n",
        "vocab = {}\n",
        "\n",
        "for sentence in corpus:\n",
        "    for word in sentence:\n",
        "        # TODO\n",
        "        pass\n",
        "\n",
        "\n",
        "# Tests\n",
        "assert len(vocab) == 19161, \"형태소의 총 개수는 19161개가 되어야합니다.\"\n",
        "assert max(vocab.values()) == 21432, \"가장 자주 등장하는 형태소의 등장 횟수는 21432번이 되어야합니다.\"\n",
        "print(\"All tests passed\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN4WBeU4lmUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "각 형태소의 등장 빈도를 꺾은선 그래프로 표현합니다.\n",
        "\"\"\"\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "matplotlib.rcParams['figure.figsize'] = [10, 8]\n",
        "\n",
        "vocab_sorted = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
        "freqs = list(zip(*vocab_sorted))[1]\n",
        "\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(freqs)\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.plot(freqs[:200])\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.plot(freqs[3000:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0RRCy-UmJ27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO\n",
        "word2idx 변수가 등장빈도가 높은 순서대로 각 형태소를 key로,\n",
        "해당 형태소의 순위를 (가장 높은 순서가 0순위입니다) value로 가지도록 하되\n",
        "상위 1%와 하위 70%는 key에서 제거해주세요.\n",
        "즉, 높은 순서대로 정렬하였을 때 순위가 1% 초과, 30% 미만에 속하는 형태소들만 key로 가지도록 구현해주시면 됩니다.\n",
        "\"\"\"\n",
        "from collections import OrderedDict\n",
        "import pickle\n",
        "\n",
        "\n",
        "word2idx = OrderedDict({})\n",
        "# TODO\n",
        "\n",
        "\n",
        "# Tests\n",
        "assert len(word2idx) == 4982, \"word2idx의 총 길이는 4982가 되어야합니다.\"\n",
        "assert list(word2idx.keys())[0] == '우리', \"word2idx의 첫번째 key는 '우리'입니다.\"\n",
        "assert list(word2idx.keys())[-1] == '객', \"word2idx의 마지막 key는 '객'입니다.\"\n",
        "print(\"All tests passed\")\n",
        "\n",
        "# Save dict\n",
        "with open('word2idx.pkl', 'wb') as handle:\n",
        "    pickle.dump(word2idx, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if1gDl45tvls",
        "colab_type": "text"
      },
      "source": [
        "## Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diP7UxADvGfX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Skip-gram based Word2Vec를 위한 데이터셋을 생성합니다.\n",
        "\"\"\"\n",
        "import torch\n",
        "import random\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "WINDOW_SIZE = 4\n",
        "UNK = len(word2idx)  # 하나의 UNK token을 사용합니다.\n",
        "\n",
        "\n",
        "class SkipGramData(Dataset):\n",
        "    def __init__(self, corpus, word2idx, num_negatives=10):\n",
        "        self.corpus = [doc for doc in corpus if len(doc) >= 2]\n",
        "        self.word2idx = word2idx\n",
        "        self.num_negatives = num_negatives\n",
        "\n",
        "    def __len__(self):\n",
        "        # document의 개수를 출력합니다.\n",
        "        return len(self.corpus)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        doc = self.corpus[idx]\n",
        "        center_idx = random.randint(0, len(doc) - 1)\n",
        "        center_word = doc[center_idx]\n",
        "        if center_word in self.word2idx:\n",
        "            center = self.word2idx[center_word]\n",
        "        else:\n",
        "            center = UNK\n",
        "        \n",
        "        # positive sample\n",
        "        lidx = max(0, center_idx - WINDOW_SIZE)\n",
        "        ridx = min(len(doc), center_idx + WINDOW_SIZE + 1)\n",
        "        pos_words = doc[lidx:center_idx] + doc[center_idx + 1:ridx]\n",
        "        context_word = random.choice(pos_words)\n",
        "        if context_word in self.word2idx:\n",
        "            pos = self.word2idx[context_word]\n",
        "        else:\n",
        "            pos = UNK\n",
        "\n",
        "        # negative samples\n",
        "        negs = []\n",
        "        for _ in range(self.num_negatives):\n",
        "            doc_idx = random.randint(0, len(self.corpus) - 1)\n",
        "            # same document\n",
        "            if doc_idx == idx:\n",
        "                doc = self.corpus[idx]\n",
        "                neg_words = doc[:lidx + 1] + doc[ridx:]\n",
        "                neg_word = random.choice(neg_words)\n",
        "            # different document\n",
        "            else:\n",
        "                doc = self.corpus[doc_idx]\n",
        "                neg_word = random.choice(doc)\n",
        "            \n",
        "            if neg_word in self.word2idx:\n",
        "                negs.append(self.word2idx[neg_word])\n",
        "            else:\n",
        "                negs.append(UNK)\n",
        "\n",
        "        return torch.LongTensor([center, pos] + negs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgMg_b_qpBky",
        "colab_type": "text"
      },
      "source": [
        "Word2Vec에서 center word $C$가 주어졌을 때, outside word (context word) $O$가 가지는 조건부 확률 분포는 아래와 같습니다.\n",
        "\n",
        "$$\n",
        "P(O=o | C=c ) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w \\in \\text{Vocab}}\\exp(u_w^T v_c)}\n",
        "$$\n",
        "\n",
        "이때 negative sampling을 하는 경우, 아래의 목적함수를 최소화하는 것으로 위 조건부 확률을 간접적으로 최대화할 수 있습니다.\n",
        "\n",
        "$$\n",
        "\\mathbf{\\it{J}}_\\text{neg-sample}(v_c, o, U) = -\\log(\\sigma(u_o^T v_c)) - \\sum_{k=1}^K \\log(\\sigma(-u_k^T v_c))\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxTc_RcniQev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO\n",
        "Word2Vec class를 구현해주세요.\n",
        "위 수식을 참조하여,\n",
        "forward() 함수를 통해 앞서 작성한 SkipGramData 데이터셋으로부터 하나의 배치가 들어왔을 때\n",
        "해당 배치의 negative sampling loss (J_neg-sample)를 반환하도록 해주세요.\n",
        "loss는 스칼라값이어야 합니다.\n",
        "\"\"\"\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Word2Vec(nn.Module):\n",
        "    def __init__(self, vocab_dim, embed_dim=10):\n",
        "        super().__init__()\n",
        "        self.W_center = nn.Embedding(vocab_dim, embed_dim)\n",
        "        self.W_context = nn.Embedding(vocab_dim, embed_dim)\n",
        "    \n",
        "    def embed(self, idx):\n",
        "        return self.W_center(idx)\n",
        "\n",
        "    def forward(self, samples):\n",
        "        # TODO\n",
        "        # Note: input의 형태에 대해서는 아래 셀을 참조해주세요.\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SKwp4Zrqxky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Word2Vec를 학습시킵니다.\n",
        "\"\"\"\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "\n",
        "learning_rate = 1e-3\n",
        "epochs = 100  # 가능하다면 수렴할 때까지 더 늘리시면 좋습니다 :)\n",
        "batch_size = 64\n",
        "\n",
        "data = SkipGramData(corpus, word2idx, num_negatives=10)\n",
        "dataloader = DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "model = Word2Vec(len(word2idx) + 1, embed_dim=10)\n",
        "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "\n",
        "pbar = tqdm(total=epochs * len(dataloader))\n",
        "for epoch in range(epochs):\n",
        "    avg_loss = []\n",
        "    for samples in dataloader:\n",
        "        model.zero_grad()\n",
        "        loss = model(samples)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        \n",
        "        avg_loss.append(loss.item())\n",
        "        pbar.update(1)\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print('Loss for epoch {}: {}'.format(\n",
        "            epoch + 1,\n",
        "            np.mean(avg_loss)\n",
        "        ))\n",
        "\n",
        "# Save model\n",
        "torch.save(model, 'word2vec.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLy9rtbcvSsr",
        "colab_type": "text"
      },
      "source": [
        "## GloVe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oipPPceZvUAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Co-occurence Matrix를 생성합니다.\n",
        "\"\"\"\n",
        "num_words = len(word2idx) + 1\n",
        "M = np.zeros((num_words, num_words))\n",
        "\n",
        "for doc in corpus:\n",
        "    current_idx = 0\n",
        "    doc_len = len(doc)\n",
        "    while current_idx < doc_len:\n",
        "        lidx = max(current_idx - WINDOW_SIZE, 0)\n",
        "        ridx = min(current_idx + WINDOW_SIZE + 1, doc_len)\n",
        "        context_words = doc[lidx:current_idx] + doc[current_idx + 1:ridx]\n",
        "        center_word = doc[current_idx]\n",
        "        if center_word in word2idx:\n",
        "            center_idx = word2idx[center_word]\n",
        "        else:\n",
        "            center_idx = UNK\n",
        "            \n",
        "        for context_word in context_words:\n",
        "            if context_word in word2idx:\n",
        "                context_idx = word2idx[context_word]\n",
        "            else:\n",
        "                context_idx = UNK\n",
        "            M[context_idx, center_idx] += 1\n",
        "        \n",
        "        current_idx += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WEkCT0qYkQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "GloVe 모델을 위한 데이터셋을 생성합니다.\n",
        "\"\"\"\n",
        "class GloVeData(Dataset):\n",
        "    def __init__(self, corpus, word2idx):\n",
        "        self.corpus = [word for word in doc for doc in corpus]\n",
        "        self.word2idx = word2idx\n",
        "\n",
        "    def __len__(self):\n",
        "        # 전체 단어의 개수를 출력합니다.\n",
        "        return len(self.corpus)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        word = self.corpus[idx]\n",
        "        if word in self.word2idx:\n",
        "            return torch.LongTensor([self.word2idx[word]])\n",
        "        else:\n",
        "            return torch.LongTensor([UNK])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Qh-fB1DAsJV",
        "colab_type": "text"
      },
      "source": [
        "GloVe 모델에서의 손실함수는 아래와 같습니다. 여기서 $X_{ij}$는 $i$번째 단어와 $j$번째 단어가 동시에 출현한 (co-occurence) 횟수입니다.\n",
        "\n",
        "$$\n",
        "\\begin{gather}\n",
        "\\sum_{i, j \\in \\text{Vocab}} f(X_{ij})(w_i^T \\tilde{w}_j + b_i + \\tilde{b}_j - \\log{X_{ij}})^2 \\\\\n",
        "f(X_{ij}) = \\begin{cases}\n",
        "    (x/x_{max})^\\alpha & \\text{if } x < x_{max} \\\\\n",
        "    1 & \\text{otherwise}\n",
        "\\end{cases} \n",
        "\\end{gather}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAfWsPou_xrw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO\n",
        "GloVe class를 구현해주세요.\n",
        "위 수식을 참조하여,\n",
        "수식에서의 f에 해당하는 weight_fn()을 구현하고\n",
        "forward() 함수를 통해 하나의 배치가 들어왔을 때\n",
        "해당 배치의 loss를 반환하도록 해주세요.\n",
        "loss는 스칼라값이어야 합니다.\n",
        "\"\"\"\n",
        "class GloVe(nn.Module):\n",
        "    def __init__(self, cooccurence, embed_dim=10, x_max=100, alpha=0.75):\n",
        "        super().__init__()\n",
        "        self.x_max = x_max\n",
        "        self.alpha = alpha\n",
        "        self.X = torch.FloatTensor(cooccurence + 1.0)\n",
        "\n",
        "        self.W_in = nn.Embedding(len(self.X), embed_dim)\n",
        "        self.W_out = nn.Embedding(len(self.X), embed_dim)\n",
        "        self.b_in = nn.Embedding(len(self.X), 1)\n",
        "        self.b_out = nn.Embedding(len(self.X), 1)\n",
        "\n",
        "    def embed(self, idx):\n",
        "        return self.W_in(idx) + self.W_out(idx)\n",
        "\n",
        "    def weight_fn(self, x):\n",
        "        # TODO\n",
        "        pass\n",
        "\n",
        "    def forward(self, sample_in, sample_out):\n",
        "        # TODO\n",
        "        # Note: input의 형태에 대해서는 아래 셀을 참조해주세요.\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pw7bcJ52bmAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "GloVe를 학습시킵니다.\n",
        "\"\"\"\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "\n",
        "learning_rate = 1e-3\n",
        "epochs = 10 # 가능하다면 수렴할 때까지 더 늘리시면 좋습니다 :)\n",
        "batch_size = 64  # 2의 배수가 되도록 설정해주세요\n",
        "\n",
        "data = GloVeData(corpus, word2idx)\n",
        "dataloader = DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "model = GloVe(M, embed_dim=10)\n",
        "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "\n",
        "pbar = tqdm(total=epochs * len(dataloader))\n",
        "for epoch in range(epochs):\n",
        "    avg_loss = []\n",
        "    for samples in dataloader:\n",
        "        model.zero_grad()\n",
        "        loss = model(\n",
        "            samples[:int(batch_size / 2)].squeeze(-1),\n",
        "            samples[int(batch_size / 2):].squeeze(-1)\n",
        "        )\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        \n",
        "        avg_loss.append(loss.item())\n",
        "        pbar.update(1)\n",
        "\n",
        "    print('Loss for epoch {}: {}'.format(\n",
        "        epoch + 1,\n",
        "        np.mean(avg_loss)\n",
        "    ))\n",
        "\n",
        "# Save model\n",
        "torch.save(model, 'glove.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2LEI3xTm7cq",
        "colab_type": "text"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAnj6hFseKfC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "학습한 Word2Vec과 GloVe 모델들이 생성하는 임베딩을 비교해봅시다.\n",
        "\"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "\n",
        "# Load models\n",
        "with open('word2idx.pkl', 'rb') as handle:\n",
        "    word2idx = pickle.load(handle)\n",
        "w2v = torch.load('word2vec.pth')\n",
        "glove = torch.load('glove.pth')\n",
        "\n",
        "# Create embeddings\n",
        "embed_w2v = {}\n",
        "embed_glove = {}\n",
        "for k, v in word2idx.items():\n",
        "    idx = torch.LongTensor([v])\n",
        "    embed_w2v[k] = w2v.embed(idx)\n",
        "    embed_glove[k] = glove.embed(idx)\n",
        "\n",
        "\n",
        "def close_words(embed_dict, query, k=20):\n",
        "    assert query in embed_dict, \"해당 형태소가 임베딩에 없습니다.\"\n",
        "    query = embed_dict[query]\n",
        "    keys = list(embed_dict.keys())\n",
        "    dists = []\n",
        "    for key in keys:\n",
        "        value = embed_dict[key]\n",
        "        cosine_sim = (query * value).sum() / query.norm() / value.norm()\n",
        "        dists.append(cosine_sim.item())\n",
        "    sim_words = [x for _, x in reversed(sorted(zip(dists, keys)))][:k + 1]\n",
        "    vectors = torch.stack([query[0].detach()] + [embed_dict[x][0].detach() for x in sim_words])\n",
        "    return vectors, sim_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VDZFSeopQJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# '멋지'와 가까운 형태소들을 찾아봅니다.\n",
        "target = '멋지'\n",
        "vectors_w2v, labels_w2v = close_words(embed_w2v, target)\n",
        "vectors_glove, labels_glove = close_words(embed_glove, target)\n",
        "\n",
        "# 2차원으로 차원을 축소합니다.\n",
        "svd = TruncatedSVD(n_components=2, n_iter=10)\n",
        "reduced_w2v = svd.fit_transform(np.asarray(vectors_w2v))\n",
        "reduced_w2v -= reduced_w2v[0]\n",
        "reduced_glove = svd.transform(np.asarray(vectors_glove))\n",
        "reduced_glove -= reduced_glove[0]\n",
        "\n",
        "# Word2Vec\n",
        "plt.scatter(0, 0, marker='x', color='red', label='word2vec')\n",
        "for idx, word in enumerate(labels_w2v[1:]):\n",
        "    x, y = reduced_w2v[idx]\n",
        "    plt.scatter(x, y, marker='x', color='red')\n",
        "    plt.text(x, y, word, fontsize=15)\n",
        "\n",
        "# GloVe\n",
        "plt.scatter(0, 0, marker='o', color='blue', label='glove')\n",
        "for idx, word in enumerate(labels_glove[1:]):\n",
        "    x, y = reduced_glove[idx]\n",
        "    plt.scatter(x, y, marker='o', color='blue')\n",
        "    plt.text(x, y, word, fontsize=15)\n",
        "\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}